{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd59505b",
   "metadata": {},
   "source": [
    "✅ go into a folder (like data/)\n",
    "✅ find files (pdf, txt, csv, xlsx, docx, json)\n",
    "✅ read them\n",
    "✅ convert them into LangChain Document objects\n",
    "✅ return one big list: documents\n",
    "========================================================\n",
    "That list is what you later split into chunks → create embeddings → store in FAISS/Chroma.\n",
    "\n",
    "1) What is a “Document” in LangChain?\n",
    "\n",
    "A LangChain Document is basically:\n",
    "\n",
    "page_content → the text\n",
    "\n",
    "metadata → extra info (file name, page number, etc.)\n",
    "\n",
    "Example (conceptually):\n",
    "\n",
    "Document(\n",
    "  page_content=\"Sheikh Mujibur Rahman was ...\",\n",
    "  metadata={\"source\": \"data/Sheikh_Mujibur_Rahman.pdf\", \"page\": 3}\n",
    ")\n",
    "\n",
    "\n",
    "For a PDF, you usually get one Document per page.\n",
    "\n",
    "So if your PDF has 51 pages:\n",
    "➡️ you get 51 Document objects.\n",
    "==============================================\n",
    "2) What your function does (high level)\n",
    "Function name\n",
    "def load_all_documents(data_dir: str) -> List[Any]:\n",
    "\n",
    "\n",
    "data_dir is the folder name (\"data\")\n",
    "\n",
    "It returns List[Any] (a list of documents).\n",
    "(Better: List[Document], but beginner-friendly is fine.)\n",
    "\n",
    "==============================================\n",
    "3) Path and why you use it\n",
    "data_path = Path(data_dir).resolve()\n",
    "\n",
    "\n",
    "Path(\"data\") means “a folder named data”.\n",
    ".resolve() converts it to the full absolute path.\n",
    "\n",
    "Example:\n",
    "\n",
    "you pass \"data\"\n",
    "\n",
    "it becomes something like:\n",
    "C:\\Users\\miraa\\workspace\\RAG\\RAG_ONE\\data\n",
    "\n",
    "That’s why you print:\n",
    "\n",
    "print(f\"[DEBUG] Data path: {data_path}\")\n",
    "\n",
    "\n",
    "So you can confirm you’re loading from the correct place.\n",
    "\n",
    "4) How it finds files (the most important line)\n",
    "\n",
    "Example for PDFs:\n",
    "\n",
    "pdf_files = list(data_path.glob('**/*.pdf'))\n",
    "\n",
    "What glob('**/*.pdf') means\n",
    "\n",
    "*.pdf = any file ending with .pdf\n",
    "\n",
    "**/ = search recursively (inside subfolders too)\n",
    "\n",
    "So it finds:\n",
    "\n",
    "data/a.pdf\n",
    "\n",
    "data/books/history.pdf\n",
    "\n",
    "data/pdf/Sheikh_Mujibur_Rahman.pdf\n",
    "\n",
    "Everything.\n",
    "\n",
    "Same idea for txt/csv/xlsx/docx/json.\n",
    "\n",
    "5) How it loads each file type\n",
    "PDF part\n",
    "loader = PyPDFLoader(str(pdf_file))\n",
    "loaded = loader.load()\n",
    "documents.extend(loaded)\n",
    "\n",
    "\n",
    "What happens:\n",
    "\n",
    "Create a loader for the PDF\n",
    "\n",
    "load() reads it\n",
    "\n",
    "loaded is a list of Documents (often 1 per page)\n",
    "\n",
    ".extend() adds all of them into your main list\n",
    "\n",
    "✅ Example:\n",
    "If PDF = 51 pages\n",
    "loaded length = 51\n",
    "documents grows by 51\n",
    "\n",
    "TXT part\n",
    "loader = TextLoader(str(txt_file))\n",
    "loaded = loader.load()\n",
    "\n",
    "\n",
    "Usually TXT becomes one Document total.\n",
    "\n",
    "✅ Example:\n",
    "notes.txt → 1 Document\n",
    "\n",
    "CSV part\n",
    "loader = CSVLoader(str(csv_file))\n",
    "loaded = loader.load()\n",
    "\n",
    "\n",
    "Depends on loader settings, but often it makes:\n",
    "\n",
    "1 Document per row, or\n",
    "\n",
    "1 document with the whole file\n",
    "\n",
    "So if your CSV has 500 rows, you might get 500 Documents.\n",
    "\n",
    "Excel part\n",
    "loader = UnstructuredExcelLoader(str(xlsx_file))\n",
    "loaded = loader.load()\n",
    "\n",
    "\n",
    "Excel is tricky:\n",
    "\n",
    "It tries to extract text from sheets and cells\n",
    "\n",
    "Output can be messy but works for many cases\n",
    "\n",
    "If you only have tables, sometimes a better choice is to convert to CSV first.\n",
    "\n",
    "Word docx part\n",
    "loader = Docx2txtLoader(str(docx_file))\n",
    "loaded = loader.load()\n",
    "\n",
    "\n",
    "Usually 1 Document.\n",
    "\n",
    "JSON part\n",
    "loader = JSONLoader(str(json_file))\n",
    "loaded = loader.load()\n",
    "\n",
    "\n",
    "⚠️ This is the one that often fails for beginners.\n",
    "\n",
    "Because many JSON loaders need to know which key to extract text from.\n",
    "\n",
    "Example JSON:\n",
    "\n",
    "[\n",
    "  {\"title\": \"A\", \"content\": \"hello\"},\n",
    "  {\"title\": \"B\", \"content\": \"world\"}\n",
    "]\n",
    "\n",
    "\n",
    "You usually must tell it: use content.\n",
    "\n",
    "So in real life you often do something like (depends on LangChain version):\n",
    "\n",
    "JSONLoader(\n",
    "  file_path,\n",
    "  jq_schema=\".[] | .content\",\n",
    "  text_content=False\n",
    ")\n",
    "\n",
    "\n",
    "If your JSON part crashes, it’s usually because of this.\n",
    "\n",
    "6) Why you use try/except everywhere\n",
    "\n",
    "Example:\n",
    "\n",
    "try:\n",
    "    loader = PyPDFLoader(str(pdf_file))\n",
    "    loaded = loader.load()\n",
    "    documents.extend(loaded)\n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] Failed to load PDF {pdf_file}: {e}\")\n",
    "\n",
    "\n",
    "Reason:\n",
    "\n",
    "One bad file should NOT crash your whole pipeline.\n",
    "\n",
    "You want it to continue loading other files.\n",
    "\n",
    "✅ Good practice.\n",
    "\n",
    "7) Why print so many debug statements?\n",
    "\n",
    "Because in ingestion pipelines, beginners get stuck on:\n",
    "\n",
    "wrong folder path\n",
    "\n",
    "files not found\n",
    "\n",
    "one loader failing silently\n",
    "\n",
    "empty documents\n",
    "\n",
    "Your debug prints help you see:\n",
    "\n",
    "how many files found\n",
    "\n",
    "which file is being loaded\n",
    "\n",
    "how many docs came from each file\n",
    "\n",
    "total docs at end\n",
    "\n",
    "That’s perfect.\n",
    "\n",
    "8) What will docs look like in your code?\n",
    "\n",
    "You do:\n",
    "\n",
    "docs = load_all_documents(\"data\")\n",
    "print(\"Example document:\", docs[0])\n",
    "\n",
    "\n",
    "A sample docs[0] could print like:\n",
    "\n",
    "page_content='...'\n",
    "\n",
    "metadata={'source': '...pdf', 'page': 0}\n",
    "\n",
    "That metadata is SUPER important later for citations.\n",
    "\n",
    "9) Beginner examples to understand the output\n",
    "Example A: Only 1 TXT file\n",
    "\n",
    "data/a.txt contains: “Hello world”\n",
    "\n",
    "Result:\n",
    "\n",
    "docs length = 1\n",
    "\n",
    "docs[0].page_content = “Hello world”\n",
    "\n",
    "docs[0].metadata[\"source\"] might contain the file path\n",
    "\n",
    "Example B: 2 PDFs (your case)\n",
    "\n",
    "history.pdf = 51 pages\n",
    "\n",
    "mujib.pdf = 43 pages\n",
    "\n",
    "Result:\n",
    "\n",
    "total docs = 51 + 43 = 94 ✅ (exactly what you saw)\n",
    "\n",
    "Example C: PDF inside subfolder\n",
    "\n",
    "If you have:\n",
    "data/pdf/book.pdf\n",
    "\n",
    "Your glob finds it because of **/*.pdf\n",
    "\n",
    "10) Important improvement (so your FAISS results show file/page)\n",
    "\n",
    "Right now, many loaders store metadata keys like:\n",
    "\n",
    "\"source\" not \"source_file\"\n",
    "\n",
    "\"page\" for pdf\n",
    "\n",
    "To make everything consistent, after loading each doc you can set:\n",
    "\n",
    "for d in loaded:\n",
    "    d.metadata[\"source_file\"] = pdf_file.name\n",
    "\n",
    "\n",
    "Do that for each file type too.\n",
    "\n",
    "This way your search results always show:\n",
    "\n",
    "source_file\n",
    "\n",
    "file_type\n",
    "\n",
    "page (for pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60618b53",
   "metadata": {},
   "source": [
    "PDF Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0dc2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import List, Any\n",
    "from langchain_community.document_loaders import PyPDFLoader, TextLoader, CSVLoader\n",
    "from langchain_community.document_loaders import Docx2txtLoader\n",
    "from langchain_community.document_loaders.excel import UnstructuredExcelLoader\n",
    "from langchain_community.document_loaders import JSONLoader\n",
    "\n",
    "def load_all_documents(data_dir: str) -> List[Any]:\n",
    "    \"\"\"\n",
    "    Load all supported files from the data directory and convert to LangChain document structure.\n",
    "    Supported: PDF, TXT, CSV, Excel, Word, JSON\n",
    "    \"\"\"\n",
    "    # Use project root data folder\n",
    "    data_path = Path(data_dir).resolve()\n",
    "    print(f\"[DEBUG] Data path: {data_path}\")\n",
    "    documents = []\n",
    "\n",
    "    # PDF files\n",
    "    pdf_files = list(data_path.glob('**/*.pdf'))\n",
    "    print(f\"[DEBUG] Found {len(pdf_files)} PDF files: {[str(f) for f in pdf_files]}\")\n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"[DEBUG] Loading PDF: {pdf_file}\")\n",
    "        try:\n",
    "            loader = PyPDFLoader(str(pdf_file))\n",
    "            loaded = loader.load()\n",
    "            print(f\"[DEBUG] Loaded {len(loaded)} PDF docs from {pdf_file}\")\n",
    "            documents.extend(loaded)\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed to load PDF {pdf_file}: {e}\")\n",
    "\n",
    "    # TXT files\n",
    "    txt_files = list(data_path.glob('**/*.txt'))\n",
    "    print(f\"[DEBUG] Found {len(txt_files)} TXT files: {[str(f) for f in txt_files]}\")\n",
    "    for txt_file in txt_files:\n",
    "        print(f\"[DEBUG] Loading TXT: {txt_file}\")\n",
    "        try:\n",
    "            loader = TextLoader(str(txt_file))\n",
    "            loaded = loader.load()\n",
    "            print(f\"[DEBUG] Loaded {len(loaded)} TXT docs from {txt_file}\")\n",
    "            documents.extend(loaded)\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed to load TXT {txt_file}: {e}\")\n",
    "\n",
    "    # CSV files\n",
    "    csv_files = list(data_path.glob('**/*.csv'))\n",
    "    print(f\"[DEBUG] Found {len(csv_files)} CSV files: {[str(f) for f in csv_files]}\")\n",
    "    for csv_file in csv_files:\n",
    "        print(f\"[DEBUG] Loading CSV: {csv_file}\")\n",
    "        try:\n",
    "            loader = CSVLoader(str(csv_file))\n",
    "            loaded = loader.load()\n",
    "            print(f\"[DEBUG] Loaded {len(loaded)} CSV docs from {csv_file}\")\n",
    "            documents.extend(loaded)\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed to load CSV {csv_file}: {e}\")\n",
    "\n",
    "    # Excel files\n",
    "    xlsx_files = list(data_path.glob('**/*.xlsx'))\n",
    "    print(f\"[DEBUG] Found {len(xlsx_files)} Excel files: {[str(f) for f in xlsx_files]}\")\n",
    "    for xlsx_file in xlsx_files:\n",
    "        print(f\"[DEBUG] Loading Excel: {xlsx_file}\")\n",
    "        try:\n",
    "            loader = UnstructuredExcelLoader(str(xlsx_file))\n",
    "            loaded = loader.load()\n",
    "            print(f\"[DEBUG] Loaded {len(loaded)} Excel docs from {xlsx_file}\")\n",
    "            documents.extend(loaded)\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed to load Excel {xlsx_file}: {e}\")\n",
    "\n",
    "    # Word files\n",
    "    docx_files = list(data_path.glob('**/*.docx'))\n",
    "    print(f\"[DEBUG] Found {len(docx_files)} Word files: {[str(f) for f in docx_files]}\")\n",
    "    for docx_file in docx_files:\n",
    "        print(f\"[DEBUG] Loading Word: {docx_file}\")\n",
    "        try:\n",
    "            loader = Docx2txtLoader(str(docx_file))\n",
    "            loaded = loader.load()\n",
    "            print(f\"[DEBUG] Loaded {len(loaded)} Word docs from {docx_file}\")\n",
    "            documents.extend(loaded)\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed to load Word {docx_file}: {e}\")\n",
    "\n",
    "    # JSON files\n",
    "    json_files = list(data_path.glob('**/*.json'))\n",
    "    print(f\"[DEBUG] Found {len(json_files)} JSON files: {[str(f) for f in json_files]}\")\n",
    "    for json_file in json_files:\n",
    "        print(f\"[DEBUG] Loading JSON: {json_file}\")\n",
    "        try:\n",
    "            loader = JSONLoader(str(json_file))\n",
    "            loaded = loader.load()\n",
    "            print(f\"[DEBUG] Loaded {len(loaded)} JSON docs from {json_file}\")\n",
    "            documents.extend(loaded)\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed to load JSON {json_file}: {e}\")\n",
    "\n",
    "    print(f\"[DEBUG] Total loaded documents: {len(documents)}\")\n",
    "    return documents\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    docs = load_all_documents(\"data\")\n",
    "    print(f\"Loaded {len(docs)} documents.\")\n",
    "    print(\"Example document:\", docs[0] if docs else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099d30a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Any, Dict\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "from langchain_community.document_loaders import (\n",
    "    PyPDFLoader,\n",
    "    TextLoader,\n",
    "    CSVLoader,\n",
    "    Docx2txtLoader,\n",
    ")\n",
    "from langchain_community.document_loaders.excel import UnstructuredExcelLoader\n",
    "from langchain_community.document_loaders import JSONLoader\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Helper: standardize metadata\n",
    "# ----------------------------\n",
    "def _standardize_metadata(\n",
    "    docs: List[Document],\n",
    "    file_path: Path,\n",
    "    file_type: str,\n",
    ") -> List[Document]:\n",
    "    \"\"\"\n",
    "    Ensure every Document has consistent metadata keys.\n",
    "    This is super important for citations in RAG (FAISS/Chroma).\n",
    "    \"\"\"\n",
    "    for d in docs:\n",
    "        d.metadata = d.metadata or {}\n",
    "\n",
    "        # Common, consistent keys\n",
    "        d.metadata[\"source_path\"] = str(file_path)      # full path\n",
    "        d.metadata[\"source_file\"] = file_path.name      # file name only\n",
    "        d.metadata[\"file_type\"] = file_type             # pdf/txt/csv/xlsx/docx/json\n",
    "\n",
    "        # Keep LangChain's original \"source\" if present; otherwise add it\n",
    "        # Many loaders set metadata[\"source\"] automatically, but not always.\n",
    "        d.metadata.setdefault(\"source\", str(file_path))\n",
    "\n",
    "        # PDF loader usually sets \"page\". If not present, keep it as None.\n",
    "        d.metadata.setdefault(\"page\", None)\n",
    "\n",
    "    return docs\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# JSON loading (beginner-safe)\n",
    "# ----------------------------\n",
    "def load_json_documents(\n",
    "    json_path: Path,\n",
    "    # Choose ONE of these patterns depending on your JSON structure\n",
    "    mode: str = \"auto\",\n",
    ") -> List[Document]:\n",
    "    \"\"\"\n",
    "    Load JSON in a safe way.\n",
    "\n",
    "    JSON is tricky because it depends on structure.\n",
    "    This function provides beginner-friendly options.\n",
    "\n",
    "    mode options:\n",
    "      1) \"auto\"        -> tries to load the file as-is (may fail for some JSON)\n",
    "      2) \"list_content\"-> expects a JSON list of objects, each has a \"content\" field\n",
    "      3) \"dict_text\"   -> expects a JSON dict with a \"text\" field\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        if mode == \"auto\":\n",
    "            # NOTE: Some LangChain JSONLoader versions require jq_schema.\n",
    "            # If this fails, use one of the explicit modes below.\n",
    "            loader = JSONLoader(str(json_path))\n",
    "            return loader.load()\n",
    "\n",
    "        if mode == \"list_content\":\n",
    "            # Example JSON:\n",
    "            # [\n",
    "            #   {\"title\": \"A\", \"content\": \"hello\"},\n",
    "            #   {\"title\": \"B\", \"content\": \"world\"}\n",
    "            # ]\n",
    "            # jq_schema extracts each content field\n",
    "            loader = JSONLoader(\n",
    "                file_path=str(json_path),\n",
    "                jq_schema=\".[] | .content\",\n",
    "                text_content=False,\n",
    "            )\n",
    "            return loader.load()\n",
    "\n",
    "        if mode == \"dict_text\":\n",
    "            # Example JSON:\n",
    "            # {\"text\": \"This is the main text\", \"source\": \"x\"}\n",
    "            loader = JSONLoader(\n",
    "                file_path=str(json_path),\n",
    "                jq_schema=\".text\",\n",
    "                text_content=False,\n",
    "            )\n",
    "            return loader.load()\n",
    "\n",
    "        raise ValueError(f\"Unknown JSON mode: {mode}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to load JSON {json_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# One function to load anything\n",
    "# ----------------------------\n",
    "def _load_one_file(file_path: Path, json_mode: str = \"auto\") -> List[Document]:\n",
    "    \"\"\"\n",
    "    Load a single file into a list of Documents, based on extension.\n",
    "    \"\"\"\n",
    "    suffix = file_path.suffix.lower()\n",
    "\n",
    "    try:\n",
    "        if suffix == \".pdf\":\n",
    "            loader = PyPDFLoader(str(file_path))\n",
    "            docs = loader.load()\n",
    "            return _standardize_metadata(docs, file_path, \"pdf\")\n",
    "\n",
    "        if suffix == \".txt\":\n",
    "            # encoding=\"utf-8\" avoids many Windows encoding issues\n",
    "            loader = TextLoader(str(file_path), encoding=\"utf-8\")\n",
    "            docs = loader.load()\n",
    "            return _standardize_metadata(docs, file_path, \"txt\")\n",
    "\n",
    "        if suffix == \".csv\":\n",
    "            loader = CSVLoader(str(file_path))\n",
    "            docs = loader.load()\n",
    "            return _standardize_metadata(docs, file_path, \"csv\")\n",
    "\n",
    "        if suffix in [\".xlsx\", \".xls\"]:\n",
    "            loader = UnstructuredExcelLoader(str(file_path))\n",
    "            docs = loader.load()\n",
    "            return _standardize_metadata(docs, file_path, \"excel\")\n",
    "\n",
    "        if suffix == \".docx\":\n",
    "            loader = Docx2txtLoader(str(file_path))\n",
    "            docs = loader.load()\n",
    "            return _standardize_metadata(docs, file_path, \"docx\")\n",
    "\n",
    "        if suffix == \".json\":\n",
    "            docs = load_json_documents(file_path, mode=json_mode)\n",
    "            return _standardize_metadata(docs, file_path, \"json\")\n",
    "\n",
    "        # Unsupported extension\n",
    "        return []\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to load {file_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Load everything from a folder\n",
    "# ----------------------------\n",
    "def load_all_documents(\n",
    "    data_dir: str,\n",
    "    recursive: bool = True,\n",
    "    json_mode: str = \"auto\",\n",
    "    allowed_exts: Optional[List[str]] = None,\n",
    ") -> List[Document]:\n",
    "    \"\"\"\n",
    "    Beginner-friendly \"load everything\" function.\n",
    "\n",
    "    Parameters:\n",
    "      data_dir     : folder name/path (example: \"data\")\n",
    "      recursive    : True -> search subfolders too\n",
    "      json_mode    : \"auto\" | \"list_content\" | \"dict_text\"\n",
    "      allowed_exts : optional list like [\".pdf\", \".txt\"] to limit file types\n",
    "\n",
    "    Returns:\n",
    "      List[Document]\n",
    "    \"\"\"\n",
    "    data_path = Path(data_dir).resolve()\n",
    "    print(f\"[INFO] Data path: {data_path}\")\n",
    "\n",
    "    if not data_path.exists():\n",
    "        raise FileNotFoundError(f\"Data folder not found: {data_path}\")\n",
    "\n",
    "    if allowed_exts is None:\n",
    "        allowed_exts = [\".pdf\", \".txt\", \".csv\", \".xlsx\", \".xls\", \".docx\", \".json\"]\n",
    "    allowed_exts = [e.lower() for e in allowed_exts]\n",
    "\n",
    "    pattern = \"**/*\" if recursive else \"*\"\n",
    "    all_files = [p for p in data_path.glob(pattern) if p.is_file() and p.suffix.lower() in allowed_exts]\n",
    "\n",
    "    print(f\"[INFO] Found {len(all_files)} supported files\")\n",
    "    for p in all_files:\n",
    "        print(f\"  - {p.name}\")\n",
    "\n",
    "    documents: List[Document] = []\n",
    "\n",
    "    for f in all_files:\n",
    "        docs = _load_one_file(f, json_mode=json_mode)\n",
    "        if docs:\n",
    "            print(f\"[INFO] Loaded {len(docs)} docs from {f.name}\")\n",
    "            documents.extend(docs)\n",
    "        else:\n",
    "            print(f\"[WARN] No docs loaded from {f.name}\")\n",
    "\n",
    "    print(f\"[INFO] Total loaded documents: {len(documents)}\")\n",
    "    return documents\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Quick test (run this file directly)\n",
    "# ----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    docs = load_all_documents(\"data\", json_mode=\"auto\")\n",
    "    print(f\"\\nLoaded {len(docs)} documents.\")\n",
    "\n",
    "    if docs:\n",
    "        d0 = docs[0]\n",
    "        print(\"\\n--- Example Document ---\")\n",
    "        print(\"Text preview:\", d0.page_content[:300])\n",
    "        print(\"Metadata:\", d0.metadata)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-one",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
